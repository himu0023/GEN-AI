{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17bbfef-5bae-41b0-98eb-93e1946becdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f94cb0-e483-423c-b43e-3c1e7f2ceed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himu23\\AppData\\Local\\Temp\\ipykernel_28472\\90880870.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model = 'tinyllama')\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "llm = Ollama(model = 'tinyllama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd9d119-1c3a-451b-a81f-63497b9b26d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genrtae answers to a question \n",
    "question = \"When did Gandhi Died?\"\n",
    "response = llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a37f062-2a6c-4ad5-9c72-4dc098459861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gandhi died on August 18, 1948 in Bombay (now Mumbai), India, at the age of 78 years old. He had been suffering from diabetes for several years and was advised by his doctors to take a short rest after he passed away.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c33d381-4d8b-4954-b5b7-f2da0f97592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarad Vallabhbhai Patel (1875-1950) was a prominent Indian politician who served as the first Indian home minister, and he played an essential role in the formation of India's independent government. He is best known for his leadership during India's struggle for independence from British colonial rule.\n",
      "\n",
      "Patel, who was born in Mehsana, Gujarat, attended the University of Nagpur before completing his graduation from the London School of Economics and Political Science (LSE). He went on to serve as an Indian cabinet minister in several governments, including the first and second ministries led by Nehru. During the Quit India Movement in 1942, Patel was appointed a member of the All-India Congress Committee, and he played a crucial role in organizing its campaigns against British imperialism.\n",
      "\n",
      "In addition to his political activities, Patel also served as an officer in the Indian Army during World War I. He was promoted to the rank of colonel in 1915 and rose through the ranks to become one of India's most respected military commanders during the 1920s and early 1930s. Patel's leadership and strategic thinking were instrumental in ensuring India's independence from British rule, despite having limited resources and facing numerous challenges along the way.\n",
      "\n",
      "Patel was appointed as the first home minister of independent India in 1946, following the end of World War II. He played a crucial role in overseeing India's transition to a modern and democratic government and in consolidating its independence from British colonial rule. Patel's leadership and diplomacy led to India's establishment as an independent nation in 1947, which he had campaigned for during his time in politics.\n",
      "\n",
      "Patel's legacy is marked by his role in the formation of India's independent government, but he was also a prolific writer who authored several books on political and social issues, including \"Struggle for India\" (1923) and \"India: Her Glorious Path and Future\" (1947). Sarad Vallabhbhai Patel's contributions to Indian politics and society have left a lasting impact and continue to inspire those who follow.\n"
     ]
    }
   ],
   "source": [
    "# Genrtae answers to a question \n",
    "question = \"Who was Sardar Vallabhbhai Patel?\"\n",
    "response = llm.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54dc9b-6a5d-4c03-b923-3fccd111c950",
   "metadata": {},
   "source": [
    "## Imptementing RAG for custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dc36647-acfa-4261-a8aa-20f9d9e8f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-community faiss-cpu sentence-transformers pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56b1f1e2-bf6c-4f53-8d75-9262e2a5633d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.3.7\n",
      "  Using cached langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community==0.3.7\n",
      "  Using cached langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\anaconda\\lib\\site-packages (from langchain==0.3.7) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\anaconda\\lib\\site-packages (from langchain==0.3.7) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\anaconda\\lib\\site-packages (from langchain==0.3.7) (3.11.10)\n",
      "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain==0.3.7)\n",
      "  Using cached langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.7)\n",
      "  Using cached langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.3.7)\n",
      "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from langchain==0.3.7)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [19 lines of output]\n",
      "      + D:\\Anaconda\\python.exe C:\\Users\\himu23\\AppData\\Local\\Temp\\pip-install-t5ctzxi1\\numpy_40eccce4639f466bbcd9122b8166a135\\vendored-meson\\meson\\meson.py setup C:\\Users\\himu23\\AppData\\Local\\Temp\\pip-install-t5ctzxi1\\numpy_40eccce4639f466bbcd9122b8166a135 C:\\Users\\himu23\\AppData\\Local\\Temp\\pip-install-t5ctzxi1\\numpy_40eccce4639f466bbcd9122b8166a135\\.mesonpy-rqafd34r -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\himu23\\AppData\\Local\\Temp\\pip-install-t5ctzxi1\\numpy_40eccce4639f466bbcd9122b8166a135\\.mesonpy-rqafd34r\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.2.99\n",
      "      Source dir: C:\\Users\\himu23\\AppData\\Local\\Temp\\pip-install-t5ctzxi1\\numpy_40eccce4639f466bbcd9122b8166a135\n",
      "      Build dir: C:\\Users\\himu23\\AppData\\Local\\Temp\\pip-install-t5ctzxi1\\numpy_40eccce4639f466bbcd9122b8166a135\\.mesonpy-rqafd34r\n",
      "      Build type: native build\n",
      "      Project name: NumPy\n",
      "      Project version: 1.26.4\n",
      "      C compiler for the host machine: gcc (gcc 6.3.0 \"gcc (MinGW.org GCC-6.3.0-1) 6.3.0\")\n",
      "      C linker for the host machine: gcc ld.bfd 2.28\n",
      "      C++ compiler for the host machine: c++ (gcc 6.3.0 \"c++ (MinGW.org GCC-6.3.0-1) 6.3.0\")\n",
      "      C++ linker for the host machine: c++ ld.bfd 2.28\n",
      "      Cython compiler for the host machine: cython (cython 3.0.12)\n",
      "      Host machine cpu family: x86\n",
      "      Host machine cpu: x86\n",
      "      \n",
      "      ..\\meson.build:28:4: ERROR: Problem encountered: NumPy requires GCC >= 8.4\n",
      "      \n",
      "      A full log can be found at C:\\Users\\himu23\\AppData\\Local\\Temp\\pip-install-t5ctzxi1\\numpy_40eccce4639f466bbcd9122b8166a135\\.mesonpy-rqafd34r\\meson-logs\\meson-log.txt\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install \"langchain==0.3.7\" \"langchain-community==0.3.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82c73d72-6466-4a9d-b314-1ab078ba2fe1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Ollama\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_history_aware_retriever, create_retrieval_chain\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# --- Load and split PDF ---\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain.chains'"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# --- Load and split PDF ---\n",
    "pdf_reader = PyPDFLoader(\"physhology.pdf\")\n",
    "documents = pdf_reader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# --- Create embeddings and FAISS vector store ---\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "\n",
    "# --- Create retriever from FAISS ---\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# --- Initialize TinyLlama model via Ollama ---\n",
    "llm = Ollama(model=\"tinyllama\")\n",
    "\n",
    "# --- Define condense question prompt ---\n",
    "CONDENSE_QUESTION_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the following conversation and a follow-up question,\n",
    "rephrase the follow-up question to be a standalone question.\n",
    "Chat History: {chat_history}\n",
    "Follow-up Input: {input}\n",
    "Standalone question:\n",
    "\"\"\")\n",
    "\n",
    "# --- Create history-aware retriever ---\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm,\n",
    "    retriever,\n",
    "    CONDENSE_QUESTION_PROMPT\n",
    ")\n",
    "\n",
    "# --- Create retrieval chain ---\n",
    "qa = create_retrieval_chain(history_aware_retriever, llm)\n",
    "\n",
    "# --- Example query ---\n",
    "response = qa.invoke({\n",
    "    \"chat_history\": [],\n",
    "    \"input\": \"Summarize the main psychological concepts from this PDF.\"\n",
    "})\n",
    "\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f32064cb-5bae-4114-8523-0135f484d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = PyPDFLoader(\"physhology.pdf\")\n",
    "documents = pdf_reader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 200,)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b8e981-c0b6-4fb8-834e-12ea53563946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himu23\\AppData\\Local\\Temp\\ipykernel_28472\\1410181046.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "C:\\Users\\himu23\\AppData\\Local\\Temp\\ipykernel_28472\\1410181046.py:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings using free HF Model\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "db = FAISS.from_documents(documents=chunks, embedding = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61ffc92c-45c6-4876-b91e-729a39036578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "llm = Ollama(model = 'tinyllama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67adb1ca-6a98-4a2e-bb23-f4088ecc1db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India got nuclear missiles in the early 1970s, after it signed the Nuclear Non-Proliferation Treaty (NPT) with other nations. The Indian nuclear program began in 1962 when it tested a bomb underground, but the government decided to wait until it became part of the NPT before pursuing nuclear weapons development.\n"
     ]
    }
   ],
   "source": [
    "# Generate answers to a question \n",
    "question = \"When did India get Nucler Missiles?\"\n",
    "response = llm.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63d89f76-e275-4bdf-b761-a770658d58f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.3\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d658b74a-4d7c-4929-a44b-000b2da2feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b4f387c-2382-4848-af2f-f4c42ce5c709",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_history_aware_retriever, create_retrieval_chain\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[0;32m      4\u001b[0m CONDENSE_QUESTION_PROMPT \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGiven the following conversation and a follow question, rephrase the follow up question to be a standalone question.\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m    Chat History:\u001b[39m\u001b[38;5;132;01m{chat_history}\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m    Follow up Input:\u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m    Standalone question:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain.chains'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = ChatPromptTemplate.from_template(\"\"\"Given the following conversation and a follow question, rephrase the follow up question to be a standalone question.\n",
    "    Chat History:{chat_history}\n",
    "    Follow up Input:{question}\n",
    "    Standalone question:\"\"\")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm,\n",
    "    retriever,\n",
    "    CONDENSE_QUESTION_PROMPT\n",
    ")\n",
    "\n",
    "qa = create_retrieval_chain(history_aware_retriever, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1631c87-f23b-43d9-b6ac-fa14f8018cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7594213-35b2-44b3-aa9b-e5912a9c59f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4826fb-f646-48d3-8613-8d0291bd4455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
